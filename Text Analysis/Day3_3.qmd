---
title: "Day 3_3: Word Embeddings Glove Model"
format: html
editor: visual
---

```{r}
#install.packages("text2vec")
#install.packages("stringr")
#install.packages("readtext")
#install.packages("magrittr")
#install.packages("Rtsne")
#install.packages("word2vec")
#install.packages("devtools")
#devtools::install_github("bmschmidt/wordVectors")
#install.packages("dplyr")
#install.packages("ggplot2")
#install.packages("textTinyR")
#install.packages("tm")
#install.packages("umap")
#install.packages("plotly")


library(wordVectors)          ### https://github.com/bmschmidt/wordVectors
library(text2vec)             ### https://text2vec.org/
library(readtext)
library(magrittr)
library(word2vec) ### https://www.rdocumentation.org/packages/word2vec/versions/0.4.0/topics/word2vec
library(conText)
library(stringr)
library(dplyr)
library(Rtsne)
library(ggplot2)
library(textTinyR)
library(tm) 
library(umap)
library(plotly)
```

## Data import

```{r}
data_directory <- "/Users/namigabbasov/Desktop/UDS/Workshops Summer 2024/Text Analysis/"
ungd_data <- readtext(paste0(data_directory, "UNGD/*"), 
                      docvarsfrom = "filenames", 
                      dvsep = "_", 
                      docvarnames = c("ccodealp", "session", "year"))
ungd_data <- subset(ungd_data, year ==2022)
```


## Text preprocessing and tokenization

```{r}
tokenizer_function <- function(x) {
  word_tokenizer(tolower(x))
}
ungd_tokens <- ungd_data$text |> 
  tokenizer_function()
```

## Create vocabulary and term co-occurrence matrix (TCM)

```{r}
iterator <- itoken(ungd_tokens, progressbar = FALSE)
ungd_vocabulary <- create_vocabulary(iterator) |> 
                   prune_vocabulary(term_count_min = 7L) |> 
                   filter(!str_detect(term, "\\d")) # Remove terms that contain numbers

vectorizer_function <- vocab_vectorizer(ungd_vocabulary)
term_cooccurrence_matrix <- create_tcm(iterator, vectorizer_function, skip_grams_window = 7L, skip_grams_window_context = "symmetric")
```


## Set hyperparameters and fit GloVe model

```{r}
RcppParallel::setThreadOptions(1)
set.seed(42)
glove_model <- GlobalVectors$new(rank = 100, x_max = 15) ### Set dimensions to 100
main_vectors <- glove_model$fit_transform(term_cooccurrence_matrix, 
                                          n_iter = 15, 
                                          convergence_tol = 0.001, 
                                          learning_rate = 0.15)
context_vectors <- glove_model$components
```

```{r}
### Combine main and context vectors
word_embeddings <- main_vectors + t(context_vectors)

### Save the word embeddings
saveRDS(word_embeddings, file = "/Users/namigabbasov/Desktop/UDS/Workshops Summer 2024/Text Analysis/word_embeddings_100d.rds")

### Load word embeddings
word_embeddings <- readRDS("/Users/namigabbasov/Desktop/UDS/Workshops Summer 2024/Text Analysis/word_embeddings_100d.rds")

### View the dimensions of embeddings
dim(word_embeddings)
```

## Visualization using PCA

```{r}
### Perform t-SNE
tsne_model <- Rtsne(word_embeddings, dims = 2, perplexity = 30, verbose = TRUE)
tsne_data <- as.data.frame(tsne_model$Y)
rownames(tsne_data) <- rownames(word_embeddings)
tsne_data$word <- rownames(tsne_data)

### Plot t-SNE
ggplot(tsne_data, aes(x = V1, y = V2, label = word)) +
  geom_point(alpha = 0.5) +
  geom_text(size = 2, alpha = 0.7, vjust = 1, hjust = 1) +
  ggtitle("t-SNE Visualization of Word Embeddings") +
  theme_minimal()
```

## Perform K-means clustering

```{r}
set.seed(42)
kmeans_model <- kmeans(word_embeddings, centers = 5, nstart = 20)
clusters <- as.factor(kmeans_model$cluster)

### A=add clusters to tsne_data for visualization
tsne_data$cluster <- clusters

# Plot t-SNE with clusters
ggplot(tsne_data, aes(x = V1, y = V2, color = cluster, label = word)) +
  geom_point(alpha = 0.5) +
  geom_text(size = 2, alpha = 0.7, vjust = 1, hjust = 1) +
  ggtitle("t-SNE Visualization of Word Embeddings with K-means Clusters") +
  theme_minimal() +
  scale_color_manual(values = c("red", "blue", "green", "purple", "orange"))
```

```{r}
### perform hierarchical clustering
distance_matrix <- dist(word_embeddings)
hc_model <- hclust(distance_matrix, method = "ward.D2")

### plot dendrogram
plot(hc_model, labels = FALSE, main = "Hierarchical Clustering Dendrogram", xlab = "", sub = "", cex = 0.6)
rect.hclust(hc_model, k = 5, border = 2:6)
```

## Find most similar words to given word

```{r}
### It becomes very hard to read because text data is large. 
### We can just look similar words 

### below, I define a function to most similar words. 
### you can break them down and do step-by-step as well. 


### define function to find similar words using text2vec's built-in functions
find_similar_words_builtin <- function(embeddings, word, top_n = 10) {
  if (!word %in% rownames(embeddings)) { ### if given word is not in word embeddings
    stop(paste("Word", word, "not found in embeddings.")) ### then stop. This is akin to raise in Python 
  }
  ### retrieve vector for target word: This is a simple indexing - word is just given word, comma means all columns 
  word_vector <- embeddings[word, , drop = FALSE] ### drop = FALSE mean still matrix 
  
  ### calculate cosine similarities - once we save the row of the word and save it as word_vector, we get cosine similarity
  similarities <- sim2(x = embeddings, y = word_vector, method = "cosine", norm = "l2") 
  
  ### convert similarities to vector
  similarities <- as.vector(similarities)
  names(similarities) <- rownames(embeddings)
  
  ### sort similarities and get top_n similar words, excluding target word itself
  sort(similarities, decreasing = TRUE)[2:(top_n + 1)] ### indexing starts from 1 in r.
}

### find top 10 most similar words to "peace"
similar_words_builtin <- find_similar_words_builtin(word_embeddings, "peace", top_n = 10)
print(similar_words_builtin)
```

```{r}
### convert results to data frame for plotting
similar_words_df <- data.frame(
  word = names(similar_words_builtin),
  similarity = as.numeric(similar_words_builtin)
)

### plot top 10 most similar words
ggplot(similar_words_df, aes(x = reorder(word, -similarity), y = similarity)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ggtitle("Top 10 Words Similar to 'Peace'") +
  xlab("Word") +
  ylab("Cosine Similarity") +
  theme_minimal()
```

```{r}
### find_nns returns nearest neighbors based on cosine similarity
### you can check its website: https://rdrr.io/github/prodriguezsosa/conText/man/find_nns.html

find_nns(target_embedding = word_embeddings["peace",],
         pre_trained = word_embeddings, N = 10,
         candidates = NULL, norm = "l2", stem = FALSE)
```

## Word2Vec: cbow and skip models 
```{r}
### extract text 
text = ungd_data$text


### train Word2Vec model
word2vec_cbow<- word2vec(x= text, 
                           type = "cbow", 
                           dim = 20,
                           iter = 15
                            )


### check embeddings 
embedding_cbow<- as.matrix(word2vec_cbow)
View(embedding_cbow)

### find semantically similar words 
similar<- predict(word2vec_cbow, c("war", "peace"), 
                  type = "nearest", 
                  top_n = 5) 
print(similar)
```

## Interactive visualization
```{r}
### I have drawn from this website: https://www.geeksforgeeks.org/word2vec-using-r/

### create corpus
corpus <- Corpus(VectorSource(text))

### preprocessing
corpus <- tm_map(corpus, content_transformer(tolower)) 
corpus <- tm_map(corpus, removePunctuation) 
corpus <- tm_map(corpus, removeNumbers) 
corpus <- tm_map(corpus, removeWords, stopwords("en")) 
corpus <- tm_map(corpus, stripWhitespace)

### convert to Document Term Matrix(DFM) and extract words
dtm <- DocumentTermMatrix(corpus)
words <- colnames(as.matrix(dtm))
word_list <- strsplit(words, " ")
word_list <- unlist(word_list)
word_list <- word_list[word_list != ""]

### take list of 50 words
word_list <- head(word_list, 50)


embedding_cbow<- predict(word2vec_cbow, word_list, type = "embedding")
embedding_cbow<-na.omit(embedding_cbow)

### visualize embeddings
vizualization <- umap(embedding_cbow, n_neighbors = 15, n_threads = 2)

embedding_df <- data.frame(word = rownames(embedding_cbow),  
                 x = vizualization$layout[, 1], 
                 y = vizualization$layout[, 2],  
                 stringsAsFactors = FALSE)

plot <- plot_ly(embedding_df, 
                x = ~x, 
                y = ~y, 
                type = 'scatter', 
                mode = 'text', 
                text = ~word)
plot <- plot %>% layout(title = "Visualization of CBOW Embeddings")

plot
```





```{r}
### train Word2Vec model using Skip-gram
word2vec_skip <- word2vec(x = text, 
                          type = "skip", 
                          dim = 20,
                          iter = 15,
                          window = 5)  ### specify window size

### Find semantically similar words
similar <- predict(word2vec_skip, c("war", "peace"), type = "nearest", top_n = 5)
print(similar)

### check embeddings
embedding_skip <- as.matrix(word2vec_skip)
View(embedding_skip)
```

